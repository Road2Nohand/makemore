{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Becoming a Backprop Ninja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viele ML Frameworks bieten direkt \"AutoGrad\" an. Z.B. PyTorch, TensorFlow, MXNet, etc.\n",
    "\n",
    "Wir wollen nun aber selbst eine Implementierung vornehmen, um ein echter Backpropagation Ninja zu werden.\n",
    "\n",
    "Als Beispiel verwenden wir wieder ein MLP das Trigramme der Vornamen bekommt und den 4. Buchstaben vorhersagen soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F # torch.nn.functional is a module that contains all the functions in the torch.nn library\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenvorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n"
     ]
    }
   ],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "print(len(words))\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(''.join(words)))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "char2idx = {c: i+1 for i, c in enumerate(chars)}\n",
    "char2idx['.'] = 0\n",
    "print(char2idx)\n",
    "\n",
    "idx2char = {i: c for c, i in char2idx.items()}\n",
    "print(idx2char)\n",
    "\n",
    "vocab_size = len(idx2char)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Length:  182778\n",
      "Dev   Length:  22633\n",
      "Test  Length:  22735\n"
     ]
    }
   ],
   "source": [
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + \".\":\n",
    "            ix = char2idx[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "shuffled_words = words.copy()\n",
    "random.shuffle(shuffled_words)\n",
    "n1 = int(0.8 * len(shuffled_words))\n",
    "n2 = int(0.9 * len(shuffled_words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1]) # 80% training\n",
    "print(\"Train Length: \", len(Xtr))\n",
    "Xdev, Ydev = build_dataset(words[n1:n2]) # 10% development\n",
    "print(\"Dev   Length: \", len(Xdev))\n",
    "Xte, Yte = build_dataset(words[n2:]) # 10% test\n",
    "print(\"Test  Length: \", len(Xte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0])\n",
      "---->\n",
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "print(Xtr[0]),\n",
    "print(\"---->\")\n",
    "print(Ytr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . .\n",
      "---->\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "print(idx2char[Xtr[0][0].item()], idx2char[Xtr[0][1].item()], idx2char[Xtr[0][2].item()])\n",
    "print(\"---->\")\n",
    "print(idx2char[Ytr[0].item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufbau des NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Function um Gradienten manuell zu berchnen und mit den AutoGrad's von PyTroch zu vergleichen\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item() # 1. Variante checkt ob die beiden Tensoren gleich sind\n",
    "    app = torch.allclose(dt, t.grad)    # 2. Variante checkt ob die beiden Tensoren fast gleich sind\n",
    "    maxdiff = (dt - t.grad).abs().max().item() # gibt den maximalen Unterschied zwischen den beiden Tensoren zurück\n",
    "    print(f\"{s:15s} | exact: {str(ex):5s} | approx: {str(app):5s} | maxdiff: {maxdiff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 64\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3) / ((n_embd * block_size) ** 0.5) # fan_in = n_embd * block_size, damit Gewichte normalverteilt sind, wegen Sättigung der tanh\n",
    "b1 = torch.randn((n_hidden,), generator=g) * 0.1 # eig wayne, weil durch BatchNorm Bias ersetzt\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn((vocab_size,), generator=g) * 0.1\n",
    "# BatchNorm\n",
    "bngain = torch.randn((1, n_hidden), generator=g) * 0.1 + 1.0 # auch als \"gamma\" bezeichnet, skaliert die Normalverteilung der Aktivierung, damit sie nicht zu klein wird\n",
    "bnbias = torch.randn((1, n_hidden), generator=g) * 0.1 # auch als \"beta\" bezeichnet, verschiebt die Normalverteilung der Aktivierung\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen eines Mini-Batches\n",
    "batch_size = 32\n",
    "n = batch_size\n",
    "# Parameter stehen für: low, high, size, generator -> random index im Bereich von 0 bis Xtr.shape[0], davon 32 Stück\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3734, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward Pass\n",
    " \n",
    "emb = C[Xb] # (32, 3, 10)\n",
    "embcat = emb.view(emb.shape[0], -1) # (32, 30)\n",
    "\n",
    "# Linear Layer 1\n",
    "zprebn = embcat @ W1 + b1 # (32, 64) -> Hidden Layer Pre-Activation z\n",
    "\n",
    "# BatchNorm Layer (Eingaben vom Ersten Hidden Layer werden normalisiert, damit die Aktivierungsfunktion nicht zu klein oder zu groß wird / Gradienten nicht zu klein werden, wegen der Sättigung der tanh)\n",
    "bnmeani = 1/n * zprebn.sum(dim=0, keepdim=True) # (1, 64) # von allen 32 Samples den Mittelwert berechnen\n",
    "bndiff = zprebn - bnmeani # (32, 64) # von allen 32 Samples den Mittelwert abziehen -> Abweichung\n",
    "bndiff2 = bndiff ** 2 # (32, 64) # Abweichung quadrieren -> Varianz\n",
    "bnvar = 1/(n-1) * bndiff2.sum(dim=0, keepdim=True) # (1, 64) # Varianz berechnen (n-1 ist Bessels Korrektur, weil wir sonst die Varianz tendenziell unterschätzen)\n",
    "bnvar_inv = (bnvar + 1e-5) ** -0.5 # (1, 64) # Inverse Wurzel der Varianz berechnen, für Normalisierung\n",
    "bnraw = bndiff * bnvar_inv # (32, 64)\n",
    "apreact = bngain * bnraw + bnbias # (32, 64) # Skalierung und Verschiebung der Normalisierten Werte, um die Aktivierung zu normalisieren\n",
    "# Nicht-Lineare Aktivierungsfunktion\n",
    "a = torch.tanh(apreact) # (32, 64) # tanh Aktivierungsfunktion\n",
    "\n",
    "# Linear Layer 2\n",
    "logits = a @ W2 + b2 # (32, 64) @ (64, 27) + (27,) -> (32, 27)\n",
    "\n",
    "# Cross Entropy Loss (Softmax + Negative Log Likelihood)\n",
    "logit_maxes = logits.max(dim=1, keepdim=True).values # (32, 1) # Maximalen Wert von jedem Sample berechnen\n",
    "norm_logits = logits - logit_maxes # (32, 27) # Logits normalisieren, damit es keine Overflows gibt, weil das kommt danach ja in Softmax, also exp, und sonst würden wir inf bekommen\n",
    "counts = norm_logits.exp() # (32, 27) # Exponentialfunktion auf die normalisierten Logits anwenden\n",
    "counts_sum = counts.sum(dim=1, keepdim=True) # (32, 1) # Summe der Exponentialfunktionen berechnen\n",
    "counts_sum_inv = counts_sum ** -1 # (32, 1) # Inverse der Summe berechnen\n",
    "probs = counts * counts_sum_inv # (32, 27) # Wahrscheinlichkeiten berechnen\n",
    "logprobs = probs.log() # (32, 27) # Logarithmus der Wahrscheinlichkeiten berechnen\n",
    "loss = -logprobs[range(n), Yb].mean() # (32,) # Negative Log Likelihood berechnen\n",
    "\n",
    "\n",
    "# Pytorch Autograd\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "for t in [logprobs, probs, counts_sum_inv, counts_sum, counts, norm_logits, logit_maxes, logits, a, apreact, bnraw, bnvar_inv, bndiff2, bndiff, bnmeani, zprebn, embcat, emb]:\n",
    "    t.retain_grad() # PyTorch soll die Gradienten der Tensoren speichern\n",
    "\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun gehen wir angfangen bei `loss` alle Teile der Cost-Function durch und bilden die Ableitungen von Hand. Wir machen Backpropagation vom Scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0312,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0312,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000, -0.0312,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0312,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000, -0.0312,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [-0.0312,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000, -0.0312,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0312,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0312,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000, -0.0312,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [-0.0312,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000, -0.0312,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000, -0.0312,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0312,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0312,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [-0.0312,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000, -0.0312,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000, -0.0312,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0312,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0312,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0312,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000, -0.0312,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -0.0312,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [-0.0312,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0312,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0312,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0312,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [-0.0312,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000, -0.0312,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0312,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000, -0.0312,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000, -0.0312,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "# hier sind die 32 Outputs drin aus Softmax bei den Samples und dann quasi durch 32 geteilt und alle negativ, weil wir ja die negative Log Likelihood berechnen\n",
    "# loss = -(a + b + c) / 3    <-- 3 hier weil wir durch die Summe teilen für mean()\n",
    "# dloss/da = -1/3a + -1/3b + -1/3c \n",
    "# dlass/da = -1/3\n",
    "# dloss/db = -1/3\n",
    "# dloss/dc = -1/3\n",
    "# dloss/dx = -1/x*n\n",
    "# Das gilt natürlich nun für alle 32 Werte, wobei es ja eig (32, 27) waren, die werden aber nicht verwendet wegen unserem Yb mapping, also alle \"0\", weil quasi Konstanten\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0 / n\n",
    "dlogprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approx: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# überprüfen mit Utiltiy Function cmp()\n",
    "cmp(\"logprobs\", dlogprobs, logprobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unser eigener Gradient für `logprobs`, passt genau zu dem vom PyTorch AutoGrad."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJoAAABBCAYAAAApBr37AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAABONSURBVHhe7VxZcFTXmf570b7vCxISIFZhQCwOa/DuCS4PKTuep4nfMq6y/RK/ueBhqpJ4Xu1JhXlwVZzNTpVxqsJiGwczAw4Yow3toAWtaN/Vkrpb6/zff/u0Lk1LqNWtVmPfrznc2/eee+69//nOv53TMo2Pjc+TgdWDybVdRczPh34XmpwO57wpGNL4oSJYog1xrpmmHFMLj2h2bQ0EDpDuapMAZA5xXWGamtIRzYCBVYKhwwwEBQbRDAQFJqfTuTqmU7UK30G/byAweMxkuqoabW5ujqanpon9wOA4xT8keAYASr4hKuNV02ggWF9fH/X29FJ4RDjl52+g2LgY11kDfoN7zWF3kG3cRiaTiZKSkshisbhOLgNB1oSBJRpa4hcAye7erafPzp6l/v5+OnzkML3wwguUnpGu1QNQF8XwEn3GzPQMdXR0UHlZOV2/cZ1iY2PpnXfeoaTEJFcNFzy13hoisN2Ml2LymExmCrNaaWxsDF8oLCyMT5lofk7HadQ1SOY7WITDI8PUcb+DbDYbDQ8N0507d8jhcGjyVHJV+yECM6YvAlXmZudEnc/MTNPE5KQIZN26bMrOzhayebvGKD4UHqjYxsfF0+ZNm6lobxHlb8gnKw9q+MNSBx/9NSFSAqZTYC4HBwapqqaaykrLqLq6irq7e0Stg2jR0dHiSxjwAy7xwefNzM7kQbyOYmJiyGw2a7Kd086HIvwnGqwhv+D9+/fpyv9eobHRUTKZTdTa2kpxcXFCMAjDGmYNKVX+2EIvQ5Y9rAi20HZz89p+KMJ/ovGL9/T00NWrV2l4eISjy3xKSU6h8fFx2U9PS5fR9tDEPQQSokIJeSi5sUhNrM0WRIsDrl1AydhbCTL8JhoioLLycqquqqbCwh2Utz6P7A47DQ0NUWZmBmVkZlJkZKT2bmv0kt9XaINX84FgRcQzCVH5+k00pC9KS0uETBs3bqSZ2RlqaGgg25iN4uMT2D/LEtMpEtALAULRjz4DPoMpJluz2bLg/+plqmTsrQQZfked8MU6OzspLj5OEoZtrW0SDJgt4PA8m9AJmpy0e73WKCso8MU4woRvhi24hpkXFHfkGYLFb40WGxNLcbFxHAx0UnFJMdU3NtDE5ARFhEdQV2cX9fb20ixrue8V9JoZ+/rvqw3WRui49vYOunbtGtXW1bKMe2Qf7ovD7nRVDC1YTp8+/Z+u/RUhIjKCIiMiKSIiQqZB0lLT2C/LoKysLMrJzaEdO7ZTakqq5kNASo9S3dxpLEetiqqHjsS+6tClrl8BoCXcz7YU9ITS19UfBwL8fJ7A846OjFB3VzfFcmRfsGkTyz6ZYmJjKC0tjcLDw101Qwcmu93uKSafIeqcVbklzDXXxi1K5zG5sD/LKt28XKIBuIbbG2D/r7WtlcLDwmnX7t0Lbfith/0ApKV/fvUd78wjRJ4PWItnRB4Nt9c/X4jAb3EIodgRdZPMBXFO8cJ8Bwv7a+7veqCTUFyYn8VIHaW+3j4aHh7maLaM/vCHP9KFCxdomn0QIS7aUNeo67191xdAv88dMj42zkQeoMkJO83N8AF0EqDq6IDztlGb1MfMB4CUlZ2vHRgYoLHRMc1fUtA/42pD917sDWk7CuqctxJk+B0MQKi89+BxfEwe3/UfdZxJ6t7n0sO+RlVVFXV2dco0FrSk0+kg55Rz4Tr18dzXF/1xzy2X/oF+qqio4ECmjZ1oJ83huHpeVQ/P5no+nGfNL9F0TW0tjQyPsOj4mMNOzc3N8swDA4PSgXK9+qAdfVFt6t9bf8zznGfxdl53L0DtP3DO20fVCVKxnDp1yi8fzS9ANi4tNzExQTeu3xBttnPnTvHxZmZm6d69JrJarHT4yBH2PcLIzhFsU1MTNTc1y2zE6OiopE/EL+G2ppxT1Hyvmdra2qmlpUWi4Hbex5Il1ImKjKKb396ktvY22rN7D6WkpIhp7+A6NTU1NDg4xMFNrGhgtN/Y0CiCSklNJZttjG7fvs23MVFOTo5E2bhfSUmJaOD1eevlWb2mGhaDXruo+otd562uHurYo+qtAVZGNPUi3l5C/5IKnvU8rofZKS+/TbdZy2zYkM8BRCFZ2RTbbOPcseXS0cd+fIymHFqnXvriErW0tTA5Oqjubp3M9aWlpJHFbOEorI5N7Xm6U3eHGpsa6csvvqS6O3Vyn/T0dFntcPPmTcrOXkd79uyRNAwIVFJaQn/77G9UfOsWO9cF7CPO0oXzF+jy119TQlw85a1fL9F1OT8P2sA8I1I6UVFRPBiaqZGjbczpwinH8wg831t9x1Z/Dvu4RB2HfPTn9VD1PKGu4S3k5XYzQgTLJxpegNU2Jsqrq6tlPRRSGp0d9+m+q2jHeN+z8LnOjk6tDofl3d3dbIocol3CWEtNT0/Tl19+QV1sMo8/dZwyMjJEK2B2obKiUmR4/PhxIc6nn35KqaxdXvvZa7Rr1y6JvC5e/JyysrMkafzJXz+RqbAXX3yRDuw/ILMW0GQ/e/VVysvPo2+++Ybq6+vp2NGjQjwsaYqICJf9qOgo0ViI3gYHB9mcEm3I3yCRc2ZmpogBPlkVvz+iu9y8XLnewWa1vqGe+9VEBQUFD83rYrkUNDDSPT0sPyk9Wunu6ZZtf28/jfPAQuDjjho9icLfsZD0Tt1d0dJdnZA/y5X7AbLv5G0/+5EQGOaY3YQPAbBElg9M2ra0NNPly1+z9tBegi2wbJcLjDYsGdq2bbuYKHTu5MQktba00jT7Zemp3PkYjVJXNkI6dGYdaysQ6+S/nhTThVUMe4r20NnPPpMcEtIr95ruCSly1+cKIeNZ68C/QvoFaG9vl7VbyWwyLVaLaFNoteTkZDHZBQWbhdxbt26l5557TgioOi3MGiYEQwd38WAB8GxY0InBgjVic6wJRSQ6kgwNDNG1b66JeQUZ5bTuPN4TETXSQs8+/ayseFkMGLiXL/+DnA6nPBPkCcCPxI0TExPp0MFDCytuPcm6RjBNTk5qT7occE2n0ylFvaAecuwRrSm1jlELQVi587q7u+i9/3qPEhMS6dSp06xRoqVuU+M9+uj3v5f6b7zxBp09e5ZNbDn9+le/pvyN+VKnpbmF3n77bTpw4En66U9P0pkzZ0SzvfXWW+J//fa/f0uRUZH0i1/8B5m5dz/44ANJKJ8+fVo6RZ4Hvc7/ED1+9NFHrPX+ST//+b/TT/7lJ0JmAHWQcqmtraH3fvMenXjpBL3++utyDn7l+++/L/f95Tu/lCS2yeLqYZYHFhj0cp3ZGS1xLXLCLV1sU6KMYUJjcKAdgReFhOVYGDgIlNT1gEY0NlFWM0WGR8rggXzVu601fIs6+QPBwzeJT4h/qCQkJlBC0tIlMTlR6kGToS3IYYY7QKZUkDNgLNxTOwaTDUBw0ECYT8V5NQ0j1/MWmvIIBw3ojNKSUpkeO3T4EJ08eZIDhmjRmLhWYc7VrrwZ78Mvw6pVECIiQuts3AcdhTr8Rb6jE3FPuc51Hs8pbbs6Vnt+qeHShhwk8FYbXFYJGrAPv9LKWwQ62rSd7lovBWYZ8ocs4xNZ7q6SCNlywdKssIgw0dbSlusZ17qsuRGHtoqKihbzND3FHcUPpQAzgw86JJYd8aysbOk0mL9ZjkihYXrYZwFh4NdlZmSKFtq2fRs1c8QJ83LwRwfFbwJxoplsiFBBEkSLuBfacNqdkr+Dn4PzMJctzc1kZxOr1xoAyAhCK/MGIeJZzOyrRfN7mMEyD8Cv+5qDiosXL9DFzy9KQW7w3LnzdO78OSnnz5+nq/93lfr6+1xXBQAPP8qaIbBEA0cWK0sA2gadO89aaWQEOSoGXzPKTvSkfZJsbHowaVxYWCjLkL67+R1HevfEX6youE15eXlUVFTEIzqRzW4ssTvARNJyb6LFmFjgC0wa7gNS4NdD6IjR0REqLSulf7DfU1xczI7/DtpUsImqa6rZb2wRf04Suvw8s3Ozkk6BiUd0CYDkIAe0EaJODArpYN07g5Rbt22lffv2UdHevVL27t9L+w/s47JfCs6hTnx8vOuq7xcCl0dThFKjCFt9WQx8jYXNCPwj5LrS0tMoJztHCHf9+j8lnYFzIAjWu6WmpVJbWxuVlWG5eLUQ8NVXXqEndj0hmgYR3qVLl+jmdzepiQODpsYm0ULQVCDa+MS4RL0pKamUzwTt7evliPeStIUAYP/+/dxOuCx9QkSYEJ9ASRwohHPbIB2iUvhQ+7heSkoymfj5i4tLWLN2y7XrctaJOdS/NyJJmfvlACaXS05Orq7kSKoEBUSFS+HGUnJ7zGBiR1U39vwDRj5MITQIOn25goJ/1NXVJeYFJvSZZ55lcxopa91gdgCs1EUnorORHlC/Z4RPgpwYgovBwQEJGOCcY3UvngFzpUjcHj1ylF77t9ckwr1y5Yr4Uy+deEnSKyAezCyiPgQIE+MTMjsBgCAJCQlisrEcCnk5aNCnnn5K2kdO7e/nzonGfPnllyXag8bUE+2BAbgU9D2B/TV3bAKHwBCNW0AkCo0ETYT8GNILWNnxSPC1cMsw5YQpHphE/Ni4kE2Y5/zpUh2Glb5VlVV05n/O0LFjx+gV1nIASPm7M78TMr755puSM8M9GhobKGddDu0s3Enhka68FVtIBBVgjVlFjS6AfNCimMFAQnk9vx+0KWYTQECkRjZs2CiEe+A51XeFRZ5foK+H/e8R0Vb+KiwICRJ5qwT+4Ycf0p/++Ce6cxe/M/SyLgrC0wvTjXkxN1u3bJUka2xsDE1NTzF5ZqXz3dctVuBCMVsdTHYka5FOAGFgkqHpctlEYcbBypEYfChk/hEkQHs5nA6ZzJd2GNDIyGl5YmzMJqbtySef5G2WaFNEt1gCdejQISbeer6WK+ov1ZNtOUA9ffEBEplDDnp5hRBWrtFcL4OwHlFXZWUl/fkvf5bQ/Zmnn6GjR49Khh2dAaccfg0iSDjnIAHqodMfSGDyDs4j+anOi3OtsITwIWjMPHz8yccSlaLj1doskAE+HIiiUggw8zCfIAzuAdO4aPv8XIhSVW5KHcP1CAZgQt3kxMuslibitpHSgc+JglQH5ITjsCjoC+Qlw0LwF2f+EY0BIiFZeOvWd/Txx5+IyTt46CBt2rhJVtciIw6Nd+LECeloON23y2/Tli1bpJ72ewKGh2CgZfARAsgBLo8QHsxne0eHRKPoEAkA4mIpNzdXfr+AhK07kQqgTdXuYm3r76v2sfWE/tgqEQ3aysbBDlYyI0+4ZesWmb1AAvfGjRsyZXeEfdEtmzc/7HasMVYsEjj8WJ+FpcQVHIlhEhupgpTUFOlYOPXIW2G03youlmU5ssymukZWToyzryPqXnWyR+dpJkz3eIsRQQdruFVM5NOsUZ9//nk6fPgw7XpiF0eNSZIxR5sPYRntuqHqYquKJ3xpz1fwgEbwgr+x0c9B0ldffSXTcviTCHBdMGc6zS4HBmiowfLuu+/6nt7gkYWlOlgFIaE849tvv5X1Wbt37aYd23dQOJZ2s0DSOFpsamyU0QY/B38iAXNx27ZvF20jpgxyUcVfcEeLGdaXYADPvtr347ZhLpGcxv1qa2tlcQLIVVS0lw4cOCARsXtSPhDyDBBWpNFaWltkFQRWm0J9I8UApxopBWgy+Aris3DrqazhkKnHIkO7fVKIiInwjPR00XZuYQS6k9BuMAUd6Of3Bn4fWAEMTvzNDfw+o6GhXuZ0D3NAgoQv/GLR3MF4Hh/gM9Gw8BD+ABYEFu4slPwWcloI/5F0TGUHHIJAFAifSea52AQODQ5SclKyzIlCAKLelTCUQAIpGH27wUAw7sX3UOYfwRV8YySoM9Iz3G6GO4IO5iBbBnwmGrLyt27dEsLk5+XTmM1GtTW1siICS21ANvzUDi+OMs1kgzAwgQ5CIjEKINp7AOprMDrscYWOQJDpxMS4mEnsS/6P4dZmIQbfiMYvidQBXgxTQvAX7t69S5VVlTKfB/JgjT+Smsj0w1yWcIQkf2apYLN87+FIVGAQymdgYSTkisQ0FpAWFu7U1uA135P0hiBEB6xPwQBSBljNiRQF0hrwBzB901DfIPklzBFmZrEa5/2y0lI2sddligYT3pijxPwhNBpMKHw595yggWUB1gTTdBUVlbKgE3OrmIlBUIClQ1hqDg3nNbpeY/hENGgsLLeRxTtsFkGY5JRkSR8os7l92zbJX/X19csqCiy3hpOKlRVI7MoPjbk+RiImmw2iLR/wg5GbRPClTeDniDzhnkDmCApiYmJ50PvsEa06TGwGl+c2qlpMDDj5auIcE8iSqZ6blSw1viMpqjLYKuvOfiu3wf/x9bLhLQi7srj3BwSd3IFZjjq1H2SbxS9DsAUFoAqS0ouK1NXGWmB5ROMa8kJQyfoX5328LMiFKPIhla1veQ1f8rGFXtYu4JA67N6XUazBAsItfNWA72hjDfvAd32if2DeSsKV/3n1C1RdL6cMLANKbjri6EWKzkMBuVCsIBl/f6ASoN9fI/hONAPBhReSqEMo6EAvVRaw5MngwTQ6OuqpaB8G13CbTgMGVoDlaTTml0EyA/7AMJ0GggKDaAaCAtPIyMijfTQDBvyEodEMBAUG0QwEBQbRDAQFBtEMBAUG0QwEBabh4WEj6jSw6jA0moGgwCCagaDAIJqBoMAgmoGgwDQ0NGQEAwZWGUT/D/itRp8qzzStAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun machen wir weiter mit der Ableitung von `logprobs = probs.log()`, wobei die Ableitung einfach:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs           | exact: True  | approx: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# logprobs = probs.log() # (32, 27) # Logarithmus der Wahrscheinlichkeiten berechnen\n",
    "dprobs = (1.0 / probs) * dlogprobs # Kettenregel f(g(x)) = f'(g(x)) * g'(x)\n",
    "cmp(\"probs\", dprobs, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dadurch das wir $(1.0 / probs) * dlogprobs$ haben, werden werte die Gradienten der Ausgänge mit einer gerigneren \"probs\" geboosted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.shape, counts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_sum_inv  | exact: True  | approx: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# c = a * b\n",
    "# a(3x3) * b(3x3) = c(3x3)\n",
    "# a11*b1 + a12*b1 + a13*b1\n",
    "# a21*b2 + a22*b2 + a23*b2\n",
    "# a31*b3 + a32*b3 + a33*b3 ...\n",
    "# = c(3x3)\n",
    "dcounts_sum_inv = (counts * dprobs).sum(dim=1, keepdim=True) # (32, 1)\n",
    "cmp(\"counts_sum_inv\", dcounts_sum_inv, counts_sum_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fassen wir kurz zusammen, sehen wir raum hier wieder `dprobs` verwendet wurde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approx: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approx: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approx: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0 / n\n",
    "# logprobs = probs.log()\n",
    "dprobs = (1.0 / probs) * dlogprobs\n",
    "# probs = counts * counts_sum_inv\n",
    "dcounts_sum_inv = (counts * dprobs).sum(dim=1, keepdim=True)\n",
    "\n",
    "cmp(\"logprobs\", dlogprobs, logprobs)\n",
    "cmp(\"probs\", dprobs, probs)\n",
    "cmp(\"counts_sum_inv\", dcounts_sum_inv, counts_sum_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approx: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approx: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approx: True  | maxdiff: 0.0\n",
      "dcounts         | exact: True  | approx: True  | maxdiff: 0.0\n",
      "dcounts_sum     | exact: True  | approx: True  | maxdiff: 0.0\n",
      "dnorm_logits    | exact: True  | approx: True  | maxdiff: 0.0\n",
      "dlogits         | exact: True  | approx: True  | maxdiff: 0.0\n",
      "dlogit_maxes    | exact: True  | approx: True  | maxdiff: 0.0\n",
      "dh              | exact: False | approx: False | maxdiff: 0.00974796712398529\n",
      "dW2             | exact: True  | approx: True  | maxdiff: 0.0\n",
      "db2             | exact: True  | approx: True  | maxdiff: 0.0\n",
      "da              | exact: False | approx: False | maxdiff: 0.00838204100728035\n",
      "dbngain         | exact: True  | approx: True  | maxdiff: 0.0\n",
      "dbnraw          | exact: True  | approx: True  | maxdiff: 0.0\n",
      "dbnbias         | exact: True  | approx: True  | maxdiff: 0.0\n",
      "dbnvar_inv      | exact: True  | approx: True  | maxdiff: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\High Performance\\AppData\\Local\\Temp\\ipykernel_12532\\3253760475.py:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  ex = torch.all(dt == t.grad).item() # 1. Variante checkt ob die beiden Tensoren gleich sind\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "all() received an invalid combination of arguments - got (bool), but expected one of:\n * (Tensor input, *, Tensor out)\n * (Tensor input, tuple of ints dim, bool keepdim, *, Tensor out)\n * (Tensor input, int dim, bool keepdim, *, Tensor out)\n * (Tensor input, name dim, bool keepdim, *, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m cmp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbnbias\u001b[39m\u001b[38;5;124m\"\u001b[39m, dbnbias, bnbias)\n\u001b[0;32m     42\u001b[0m cmp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbnvar_inv\u001b[39m\u001b[38;5;124m\"\u001b[39m, dbnvar_inv, bnvar_inv)\n\u001b[1;32m---> 43\u001b[0m \u001b[43mcmp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbnvar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbnvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbnvar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m cmp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbndiff2\u001b[39m\u001b[38;5;124m\"\u001b[39m, dbndiff2, bndiff2)\n",
      "Cell \u001b[1;32mIn[66], line 3\u001b[0m, in \u001b[0;36mcmp\u001b[1;34m(s, dt, t)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcmp\u001b[39m(s, dt, t):\n\u001b[1;32m----> 3\u001b[0m     ex \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m# 1. Variante checkt ob die beiden Tensoren gleich sind\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     app \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mallclose(dt, t\u001b[38;5;241m.\u001b[39mgrad)    \u001b[38;5;66;03m# 2. Variante checkt ob die beiden Tensoren fast gleich sind\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     maxdiff \u001b[38;5;241m=\u001b[39m (dt \u001b[38;5;241m-\u001b[39m t\u001b[38;5;241m.\u001b[39mgrad)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m# gibt den maximalen Unterschied zwischen den beiden Tensoren zurück\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: all() received an invalid combination of arguments - got (bool), but expected one of:\n * (Tensor input, *, Tensor out)\n * (Tensor input, tuple of ints dim, bool keepdim, *, Tensor out)\n * (Tensor input, int dim, bool keepdim, *, Tensor out)\n * (Tensor input, name dim, bool keepdim, *, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs) # loss = -logprobs[range(n), Yb].mean()\n",
    "dlogprobs[range(n), Yb] = -1.0 / n\n",
    "dprobs = (1.0 / probs) * dlogprobs # logprobs = probs.log()\n",
    "dcounts_sum_inv = (counts * dprobs).sum(dim=1, keepdim=True) # probs = counts * counts_sum_inv -> nach \"counts_sum_inv\" ableiten\n",
    "dcounts = counts_sum_inv * dprobs # probs = counts * counts_sum_inv -> nach \"counts\" ableiten\n",
    "dcounts_sum = (-counts_sum ** -2) * dcounts_sum_inv # counts_sum_inv = counts_sum ** -1 -> nach \"counts_sum\" ableiten\n",
    "# counts_sum = counts.sum(dim=1, keepdim=True) -> nach \"counts\" ableiten\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum # hier kommt += rein, weil wir quasi in einem weiteren Ableitungszweig von \"counts\" sind\n",
    "dnorm_logits = counts * dcounts # counts = norm_logits.exp() -> nach \"norm_logits\" ableiten\n",
    "# norm_logits = logits - logit_maxes -> nach \"logit_maxes\" ableiten\n",
    "dlogits = dnorm_logits.clone() # nicht das finale dlogits, sondern nur die Ableitung von \"norm_logits\" nach \"logits\", alsi wieder ein Zweig, der später noch verändert werden muss\n",
    "dlogit_maxes = -dlogits.sum(dim=1, keepdim=True)\n",
    "dlogits += F.one_hot(logits.max(dim=1).indices, num_classes=logits.shape[1]) * dlogit_maxes # logit_maxes = logits.max(dim=1, keepdim=True).values\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = a.T @ dlogits\n",
    "db2 = dlogits.sum(dim=0)\n",
    "da = (1.0 - a**2) * dh\n",
    "dbngain = (bnraw * da).sum(dim=0, keepdim=True)\n",
    "dbnraw = bngain * da\n",
    "dbnbias = da.sum(dim=0, keepdim=True)\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(dim=0, keepdim=True)\n",
    "dbnvar = (-0.5 * (bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "dbndiff2 = (1.0/(n-1)) * torch.ones_like(bndiff2) * dbnvar\n",
    "\n",
    "\n",
    "\n",
    "cmp(\"logprobs\", dlogprobs, logprobs)\n",
    "cmp(\"probs\", dprobs, probs)\n",
    "cmp(\"counts_sum_inv\", dcounts_sum_inv, counts_sum_inv)\n",
    "cmp(\"dcounts\", dcounts, counts)\n",
    "cmp(\"dcounts_sum\", dcounts_sum, counts_sum)\n",
    "cmp(\"dnorm_logits\", dnorm_logits, norm_logits)\n",
    "cmp(\"dlogits\", dlogits, logits)\n",
    "cmp(\"dlogit_maxes\", dlogit_maxes, logit_maxes)\n",
    "cmp(\"dh\", dh, zprebn)\n",
    "cmp(\"dW2\", dW2, W2)\n",
    "cmp(\"db2\", db2, b2)\n",
    "cmp(\"da\", da, a)\n",
    "cmp(\"dbngain\", dbngain, bngain)\n",
    "cmp(\"dbnraw\", dbnraw, bnraw)\n",
    "cmp(\"dbnbias\", dbnbias, bnbias)\n",
    "cmp(\"dbnvar_inv\", dbnvar_inv, bnvar_inv)\n",
    "cmp(\"dbnvar\", dbnvar, bnvar)\n",
    "cmp(\"dbndiff2\", dbndiff2, bndiff2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Er hat hier einfach backprop weitergeführt und dann in das NN eingebaut. Machen wir nicht weiter, weil pain in the ass."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
